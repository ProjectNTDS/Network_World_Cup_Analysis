{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of what is available in Data:\n",
    "\n",
    "## With Numbers of Visit\n",
    "\n",
    "### Numbers of visit in absolute scale\n",
    "- <i>All_Nodes_During_WC.csv</i>: a table with every nodes, their categories and the number of visit per day during the World cup itself (the world cup started on 14 June and lasted until 15 July 2018). This starts on the 10th of June and ends on 20th of July (border included).\n",
    "[<b>Remark: for <i>All_Nodes_During_WC.csv</i>, 2 problems occur at the nodes 209 and 572 (Peter Etebo and Ricardo Pereira). Their numbers of visits are set to 0.]\n",
    "\n",
    "\n",
    "- <i>Countries_Enlarged_During_Year.csv</i>: [<b>for question 2</b>] any countries in any world cup in history. Number of visit per month during the year of the world cup (actually: from 14 November 2017 to (included) 14 November 2018. <b>IMPORTANT REMARK</b>: to help observe the world cup, start of month is set to 14 (since the world cup started the 14th June). The column indicates the month the time started. This is done because otherwise the world cup is cut in two parts. \n",
    "\n",
    "\n",
    "- <i>Countries_Enlarged_During_Year_centred.csv</i>: same as right above except it starts at the first of the month and goes from Decembre 2017 to Novembre 2018\n",
    "\n",
    "### Numbers of visit in Normalised scale \n",
    "Same files as above but the names end with \"\\_Normalised\" \n",
    "\n",
    "\n",
    "- <i>All_Nodes_During_WC_Normalised.csv</i>: as above but normalised. <b>Last column 'Sum' (index 43) is the total value !!</b>\n",
    "\n",
    "\n",
    "- <i>Countries_Enlarged_During_Year_Normalised.csv</i>: as above but normalised. <b>Last column 'Sum' (index 14)is the total value !!</b>\n",
    "\n",
    "## Without Number of Visits\n",
    "\n",
    "- <i>All_Nodes.csv</i>: each nodes and its category <b>[useful to create a new csv but keep this one clean!]</b>\n",
    "\n",
    "\n",
    "- <i>Nodes_Linked.csv</i>: each node enters the table with its links in <i>Links</i> and the category of the node in <i>Category</i>(the original one or, equivalently, that in the <i>Nodes</i> column). <b>Use this for the connected network base on hyperlinks</b> (do check that what you obtain is connected!)\n",
    "\n",
    "\n",
    "## Supplement: \n",
    "These <b>should not be necessary</b> as they only contain the node and its category in seperated files for each category\n",
    "\n",
    "Separeted Info:\n",
    "- players in the 2018 World Cup in <i>All_players.csv</i>\n",
    "- countries in the 2018 World Cup in <i>All_countries.csv</i>\n",
    "- teams in the 2018 World Cup in All_<i>National_Teams.csv</i>\n",
    "\n",
    "And in extra, with more entries than just that World Cup: [modified for question 2 in later]\n",
    "- any countries in any world cup in <i>Countries_Enlarged.csv</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only save if you're sure the file needs to be modified ! Please contact me first.\n",
    "SAVE = 0\n",
    "SAVE_WITH_LINKS = 0\n",
    "SAVE_WITH_WC_VISITS = 0\n",
    "SAVE_WITH_WC_VISITS_NORMALISED = 0\n",
    "SAVE_COUNTRY_YEAR = 0\n",
    "SAVE_COUNTRY_YEAR_NORMALISED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pages: \n",
    "\n",
    "Players: https://en.wikipedia.org/w/index.php?title=Category:2018_FIFA_World_Cup_players&pagefrom=Layun%2C+Miguel%0AMiguel+Lay√∫n#mw-pages\n",
    "\n",
    "Countries: https://en.wikipedia.org/wiki/Category:Countries_at_the_2018_FIFA_World_Cup\n",
    "\n",
    "Teams? https://en.wikipedia.org/wiki/2018_FIFA_World_Cup_squads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get the links from the category page\n",
    "import requests\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "    'action': \"query\",\n",
    "    'list': \"categorymembers\",\n",
    "    'cmtitle': \"Category:2018 FIFA World Cup players\",\n",
    "    'cmlimit': 800,\n",
    "    'format': \"json\"\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "DATA = R.json()\n",
    "\n",
    "player_list = []\n",
    "for i in range(len(DATA['query']['categorymembers'])):\n",
    "    player_list.append(DATA['query']['categorymembers'][i]['title'])\n",
    "\"\"\"\n",
    "for player in player_list:\n",
    "    print(player)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload the last players (unfortunately method above is limited at 500)\n",
    "last_players_pd = pd.read_csv(\"./Data/Process/Players_End.csv\")\n",
    "\n",
    "# Combine Both in a new list.\n",
    "last_players = last_players_pd['Players'].values.tolist()\n",
    "player_list.extend(last_players)\n",
    "players = OrderedDict([ ('Players', player_list)])\n",
    "player_pd = pd.DataFrame.from_dict(players)\n",
    "\n",
    "if(SAVE):\n",
    "    player_pd.to_csv('./Data/Process/All_Player_Dirty.csv', sep='\\t', encoding= 'utf-16', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the All_Player_Dirty and cleaned by Google Spreadsheet. Load this\n",
    "#       player_pd  = pd.read_csv(\"./Data/All_Players.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the data (we get the countries and the teams manually):\n",
    "- players in All_players.csv\n",
    "- countries in All_countries.csv\n",
    "- teams in All_National_Teams.csv\n",
    "\n",
    "And in extra, with more entries than just that world cup:\n",
    "- any countries in any world cup in Countries_Enlarged.csv\n",
    "\n",
    "We now have to find the links on these pages and the number of visit for each. Let's make a register where we store every nodes. \n",
    "\n",
    "REMARK: Countries_Enlarged.csv is not used since the links will only be based on visit and not hyperlinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_pd  = pd.read_csv(\"./Data/All_Players.csv\")\n",
    "countries_pd  = pd.read_csv(\"./Data/All_Countries.csv\")\n",
    "teams_pd  = pd.read_csv(\"./Data/All_National_Teams.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [player_pd, countries_pd, teams_pd]\n",
    "Nodes_Join = pd.concat(frames)\n",
    "Nodes_Join.to_csv('./Data/All_Nodes.csv', sep='\\t', encoding= 'utf-16', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of these nodes, need to run command to get all pages linked, and keep only those that match one of the node.Add these into a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_links(page):\n",
    "        links = page.links\n",
    "        return links     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proccess_data_link(Nodes_Join):\n",
    "    entry_nodes = Nodes_Join.Node.tolist()\n",
    "    entry_category = Nodes_Join.Category.tolist()\n",
    "    size = len(entry_nodes)\n",
    "    final_nodes = []\n",
    "    final_link = []\n",
    "    final_category = []\n",
    "    \n",
    "    for i in range(size):\n",
    "        if(i%50 == 0):\n",
    "            print(\"At node\", i)\n",
    "        page_run = wikipedia.page(entry_nodes[i])\n",
    "        links_from_page = print_links(page_test)\n",
    "        links_from_page_in_nodes =  list(set(entry_nodes).intersection(links_from_page))\n",
    "        for elem in links_from_page_in_nodes:\n",
    "            final_nodes.append(entry_nodes[i])\n",
    "            final_link.append(elem)\n",
    "            final_category.append(entry_category[i])\n",
    "    \n",
    "    linked = OrderedDict([('Nodes', final_nodes),\n",
    "          ('Links', final_link),\n",
    "          ('Node_Category',  final_category)])\n",
    "    \n",
    "    df_linked = pd.DataFrame.from_dict(linked)\n",
    "    \n",
    "    return df_linked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  proccess_data_link(Nodes_Join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(SAVE_WITH_LINKS):\n",
    "    df.to_csv('./Data/Nodes_Linked.csv', sep='\\t', encoding= 'utf-16', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Number of Visits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: During the entire World Cup for every nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserActivity(article, granularity, start, end, project =\"en.wikipedia.org\",\n",
    "                    access=\"all-access\", agent=\"user\",dateformat=\"iso\"):\n",
    "    \"\"\"\n",
    "    Method to obtain user activity of a given page for a given period of time\n",
    "    article: name of the wikiipedia article\n",
    "    granularity: time granularity of activity, either 'monthly' or 'daily'\n",
    "    start: start date of the research as Datetime.datetime object\n",
    "    end: end date of the research as Datetime.datetime object\n",
    "    project: If you want to filter by project, use the domain of any Wikimedia project (by default en.wikipedia.org)\n",
    "    access: If you want to filter by access method, use one of desktop, mobile-app or mobile-web (by default all-access)\n",
    "    agent: If you want to filter by agent type, use one of user, bot or spider (by default user).\n",
    "    dateformat: the dateformat used in result array, can be 'iso','ordinal','datetime'.\n",
    "    return:\n",
    "        it return an array of array of the form [ [user_activity_value1, date1], [user_activity_value2, date2]]\n",
    "    \"\"\"\n",
    "\n",
    "    #granularity['monthly','daily']\n",
    "    #format['iso','ordinal','datetime']\n",
    "    #Be carefull, for daily granularity left bound date is included, for monthly granularity left bound date is excluded\n",
    "    \n",
    "    dstart = start.strftime(\"%Y%m%d\")\n",
    "    dend = end.strftime(\"%Y%m%d\")\n",
    "    path = (\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/\"+project\n",
    "            +\"/\"+access+\"/\"+agent+\"/\"+article+\"/\"+granularity+\"/\"+dstart+\"/\"+dend)\n",
    "    r = requests.get(path)\n",
    "    res = []\n",
    "    for i in range(len(r.json()['items'])):\n",
    "        time_label = None\n",
    "        if granularity == 'daily':\n",
    "            time_label = (start + datetime.timedelta(days=i))\n",
    "        else:\n",
    "            time_label = (start + relativedelta(months=+i))\n",
    "        if dateformat == 'iso':\n",
    "            time_label = time_label.isoformat()\n",
    "        elif dateformat == 'ordinal':\n",
    "            time_label = time_label.toordinal()\n",
    "            \n",
    "        res.append([r.json()['items'][i]['views'],time_label])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set of all nodes and their category. We will modify this database \n",
    "# several times to get what we want\n",
    "Nodes_Join = pd.read_csv(\"./Data/All_Nodes.csv\", sep='\\t', encoding= 'utf-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First query: visit during the World Cup (slightly extended) from  10th June till 20th July.\n",
    "s = datetime.datetime(year=2018,month=6,day=10)\n",
    "e = datetime.datetime(year=2018,month=7,day=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_query(Nodes_Join, start_time, end_time, timestep):\n",
    "    df = Nodes_Join.copy(deep=True)\n",
    "    entry_nodes = df.Node.tolist()\n",
    "    \n",
    "    #Number of Days in between:\n",
    "    #number_days = (end_time-start_time).days\n",
    "    \n",
    "    #We do case 0 here, then the rest in loop\n",
    "    node_0 = entry_nodes[0]\n",
    "    if(timestep == \"daily\"):\n",
    "        time_list = getUserActivity(article=node_0,granularity=\"daily\",\\\n",
    "                                start=start_time,end=end_time,dateformat=\"datetime\")\n",
    "    if(timestep == \"monthly\"):\n",
    "        time_list = getUserActivity(article=node_0,granularity=\"monthly\",\\\n",
    "                                start=start_time,end=end_time,dateformat=\"datetime\")\n",
    "    counter = 0\n",
    "    for elem in time_list:\n",
    "        if(timestep == \"daily\"):\n",
    "            df[elem[1].strftime('%d/%m/%Y')] = ''\n",
    "        if(timestep == \"monthly\"):\n",
    "            df[elem[1].strftime('%m/%Y')] = ''\n",
    "        df.iloc[0, 2 + counter] = elem[0]\n",
    "        counter = counter +1\n",
    "    #Case 0 done\n",
    "    err_list = []\n",
    "    #Start Loop\n",
    "    for i in range(1, len(entry_nodes)):\n",
    "        if(i % 50 == 0):\n",
    "            print(\"At Node \", i)\n",
    "        node_i = entry_nodes[i]\n",
    "        try:\n",
    "            if(timestep == \"daily\"):\n",
    "                time_list = getUserActivity(article=node_i,granularity=\"daily\",\\\n",
    "                                        start=start_time,end=end_time,dateformat=\"datetime\")\n",
    "            if(timestep == \"monthly\"):\n",
    "                time_list = getUserActivity(article=node_i,granularity=\"monthly\",\\\n",
    "                                        start=start_time,end=end_time,dateformat=\"datetime\")\n",
    "            counter = 0\n",
    "            for elem in time_list:\n",
    "                df.iloc[i, 2 + counter] = elem[0]\n",
    "                counter = counter +1\n",
    "        except:\n",
    "            counter = 0\n",
    "            for elem in time_list:\n",
    "                df.iloc[i, 2 + counter] = 0\n",
    "                counter = counter +1\n",
    "            err_list.append(i)\n",
    "            \n",
    "        \n",
    "    return df, err_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Nodes_During_WC, err_list = perform_query(Nodes_Join, s, e, \"daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem occurs at the nodes 209 and 572 (Peter Etebo and Ricardo Pereira). The values is set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(SAVE_WITH_WC_VISITS):\n",
    "    Nodes_During_WC.to_csv('./Data/All_Nodes_During_WC.csv', sep='\\t', encoding= 'utf-16', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Nodes_During_WC = pd.read_csv(\"./Data/All_Nodes_During_WC.csv\", sep='\\t', encoding= 'utf-16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Example\n",
    "All_Nodes_During_WC[All_Nodes_During_WC['Node'] == \"Lionel Messi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: During the entire year (per month) for every country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Contries_Enlarged  = pd.read_csv(\"./Data/Countries_Enlarged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note how the day is set to 14th to help visualise the world cup and concentrate it in a month\n",
    "s = datetime.datetime(year=2017,month=11,day=14)\n",
    "e = datetime.datetime(year=2018,month=12,day=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Countries_During_Year, err_list_Year = perform_query(Contries_Enlarged, s, e, \"monthly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No errors here (can check <i>err_list_Year</i>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(SAVE_COUNTRY_YEAR):\n",
    "    Countries_During_Year.to_csv('./Data/Countries_Enlarged_During_Year.csv', sep='\\t', encoding= 'utf-16', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contries_Enlarged_2  = pd.read_csv(\"./Data/Countries_Enlarged_During_Year.csv\", sep='\\t', encoding= 'utf-16')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Let's normalised these signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Nodes_During_WC = pd.read_csv(\"./Data/All_Nodes_During_WC.csv\", sep='\\t', encoding= 'utf-16')\n",
    "All_Nodes_During_WC['Sum'] = All_Nodes_During_WC.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (2, 43):\n",
    "    All_Nodes_During_WC.iloc[:, i]= All_Nodes_During_WC.iloc[:, i]/All_Nodes_During_WC.iloc[:, 43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(SAVE_WITH_WC_VISITS_NORMALISED):\n",
    "    All_Nodes_During_WC.to_csv(\"./Data/All_Nodes_During_WC_Normalised.csv\", sep='\\t', encoding= 'utf-16', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Countries_During_Year = pd.read_csv(\"./Data/Countries_Enlarged_During_Year.csv\", sep='\\t', encoding= 'utf-16')\n",
    "Countries_During_Year['Sum'] = Countries_During_Year.sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (2, 14):\n",
    "    Countries_During_Year.iloc[:, i]= Countries_During_Year.iloc[:, i]/Countries_During_Year.iloc[:, 14]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(SAVE_COUNTRY_YEAR_NORMALISED):\n",
    "    Countries_During_Year.to_csv(\"./Data/Countries_Enlarged_During_Year_Normalised.csv\", sep='\\t', encoding= 'utf-16', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Countries_During_Year_2  = pd.read_csv(\"./Data/Countries_Enlarged_During_Year_Normalised.csv\", sep='\\t', encoding= 'utf-16')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>THE END</b> (and the start for you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
